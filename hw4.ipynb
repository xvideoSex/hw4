{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3df2f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencc\n",
    "#!pip install word2vec\n",
    "#!pip install -U gensim\n",
    "#!pip install python-Levenshtein\n",
    "import json\n",
    "from opencc import OpenCC\n",
    "import jieba\n",
    "import re\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ed57ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len : 729440\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = '[）\\（\\：\\·\\，\\。\\“ \\”\\?\\？\\」\\「\\……\\、\\《\\》\\；\\)\\(]'\n",
    "file = ['AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK','AL']\n",
    "opencc = OpenCC('s2t')\n",
    "\n",
    "def open_wiki():\n",
    "    result =[]\n",
    "    for a in file[0:4]:\n",
    "        for i in range(100):\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(a,str(i).zfill(2)),encoding=\"utf-8\"):\n",
    "                data = json.loads(line)\n",
    "                value = re.sub(r,'',data['text'])\n",
    "                result.append(value)\n",
    "\n",
    "    for a in file[5:8]:\n",
    "        for i in range(100):\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(a,str(i).zfill(2)),encoding=\"utf-8\"): \n",
    "                data = json.loads(line)\n",
    "                value = re.sub(r,'',data['text'])\n",
    "                result.append(value)\n",
    "\n",
    "    for a in file[9:11]:\n",
    "        for i in range(100):\n",
    "            for line in open('wiki_zh/{}/wiki_{}'.format(a,str(i).zfill(2)),encoding=\"utf-8\"):\n",
    "                data = json.loads(line)\n",
    "                value = re.sub(r,'',data['text'])\n",
    "                result.append(value)\n",
    "\n",
    "    for i in range(74):\n",
    "        for line in open('wiki_zh/{}/wiki_{}'.format('AM',str(i).zfill(2)),encoding=\"utf-8\"):\n",
    "            data = json.loads(line)\n",
    "            value = re.sub(r,'',data['text'])\n",
    "            result.append(value)\n",
    "            \n",
    "    return result\n",
    "    \n",
    "result = open_wiki()\n",
    "\n",
    "print('len :',len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9bc7bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def chine(result):\n",
    "    value = []\n",
    "    for line in result:\n",
    "        a = opencc.convert(line)\n",
    "        value.append(a)\n",
    "    return value\n",
    "\n",
    "value=chine(result)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "686f4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopword_list():       \n",
    "    with open('wiki_zh/cn_stopwords.txt',encoding=\"utf-8\") as f: \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ee97e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\david\\anaconda3\\Lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\david\\AppData\\Local\\Temp\\jieba.u48121da74622d113f9ad80c5e03d95dc.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.697 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "def macktxt(value):\n",
    "    stop = get_stopword_list()\n",
    "    f = open('test1.txt','w',encoding=\"utf-8\")\n",
    "\n",
    "    print(len(value))\n",
    "\n",
    "    jieba.set_dictionary('/Users/david/anaconda3/Lib/site-packages/jieba/dict.txt')\n",
    "    jieba.load_userdict(\"txt.txt\")  \n",
    "\n",
    "    for line in value :  \n",
    "        data = jieba.lcut(line,cut_all=False)\n",
    "        for val in data:\n",
    "            if val not in stop:\n",
    "                f.write(val+' ')\n",
    "        # f.write('\\n')\n",
    "    f.close\n",
    "\n",
    "macktxt(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76149410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2():\n",
    "    train_data = word2vec.LineSentence('test1.txt')\n",
    "    # seed = 666\n",
    "    sg = 1\n",
    "    # window_size = 10\n",
    "    vector_size = 300\n",
    "    # min_count = 1\n",
    "    workers = 4\n",
    "    epochs = 5\n",
    "    # batch_words = 10000\n",
    "    model = word2vec.Word2Vec(\n",
    "        train_data,\n",
    "        # min_count=min_count,\n",
    "        vector_size=vector_size,\n",
    "        workers=workers,\n",
    "        epochs=epochs,\n",
    "        # window=window_size,\n",
    "        sg=sg,\n",
    "        # seed=seed,\n",
    "        # batch_words=batch_words\n",
    "    )\n",
    "    model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba8d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(string):\n",
    "    model = word2vec.Word2Vec.load('word2vec.model')\n",
    "    # print(model.wv['生物'].shape)\n",
    "\n",
    "    for item in model.wv.most_similar(string,topn=20):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2964b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練成功！\n",
      "('天龍國', 0.6773413419723511)\n",
      "('灣記者', 0.6560138463973999)\n",
      "('灣各個', 0.6520161628723145)\n",
      "('灣活動', 0.6495330929756165)\n",
      "('縣民進', 0.6493810415267944)\n",
      "('縣羅東', 0.6453896164894104)\n",
      "('灣各大', 0.6403369903564453)\n",
      "('灣多數', 0.6395366191864014)\n",
      "('澎湖人', 0.6384385228157043)\n",
      "('縣後壁', 0.637714147567749)\n",
      "('灣打', 0.6366744041442871)\n",
      "('用心看', 0.6366576552391052)\n",
      "('雲林嘉義', 0.6354460716247559)\n",
      "('北板橋', 0.6353349089622498)\n",
      "('老戲院', 0.6348544359207153)\n",
      "('雲林人', 0.6305221915245056)\n",
      "('高雄人', 0.6288862824440002)\n",
      "('灣嘉義的', 0.6275696158409119)\n",
      "('駐市', 0.6270580291748047)\n",
      "('灣大', 0.6264476180076599)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word2()\n",
    "print('訓練成功！')\n",
    "test('台灣')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6a370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
